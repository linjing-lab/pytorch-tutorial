# pytorch_tutorialğŸ”–

<p align="center">
    <img src="./asserts/logo.jpg">
</p>

<p align='center'>
    <a href='https://jupyter.org/'>
        <img src="https://img.shields.io/badge/Editor-Jupyter-informational?style=flat&logo=jupyter&logoColor=white&color=2bbc8a"> 
    </a>
    <a href="https://pytorch.org/"> 
        <img src="https://img.shields.io/badge/Framework-PyTorch-informational?style=flat&logo=pytorch&logoColor=white&color=2bbc8a"> 
    </a>
    <a href='https://www.python.org/'>
        <img src="https://img.shields.io/badge/Code-Python-informational?style=flat&logo=python&logoColor=white&color=2bbc8a">
    </a>
</p>

## å›¾åƒåˆ†ç±»

å­¦ä¹ å›¾åƒåˆ†ç±»å‰çš„å‡†å¤‡ï¼š

* [pytorchåŸºç¡€](./image_classification/pytorch_basics.ipynb)
* [çº¿æ€§å›å½’](./image_classification/linear_regression.ipynb)

### MNISTæ•°æ®é›†

å¦‚æœæœ‰äººæ•°æ®é›†èƒ½å¤Ÿæ— éšœç¢çš„ä¸‹è½½MNISTæ•°æ®é›†ï¼Œé‚£ä¹ˆï¼š

```python
# MNIST dataset (images and labels)
train_dataset = torchvision.datasets.MNIST(root='data/MNIST', train=True, transform=torchvision.transforms.ToTensor(), download=True)
test_dataset = torchvision.datasets.MNIST(root='data/MNIST', train=False, transform=torchvision.transfroms.ToTensor(), download=True)
```

å¦‚æœä¸èƒ½ï¼Œå…ˆä»MNISTæ•°æ®é›†å®˜ç½‘ä¸ŠæŠŠæ•°æ®é›†ä¸‹è½½ä¸‹æ¥, å­˜åœ¨'data/MNIST/'ä¸­ï¼ˆdataä¸é¡¹ç›®æ–‡ä»¶åŒçº§ï¼‰ï¼Œå†ä½¿ç”¨è¿™ä¸ªæ•°æ®é›†åŠ è½½ç±»è¿›è¡Œæ“ä½œã€‚

```python
import numpy as np
import gzip
import os
class MNISTDataset(torch.utils.data.Dataset):
    def __init__(self, root, train=True, transform=None):
        # The file name prefix is obtained according to whether it is a training set or not.
        self.file_pre = 'train' if train == True else 't10k'
        self.transform = transform

        # Generate the image and label file path of the corresponding dataset.
        self.label_path = os.path.join(root, '%s-labels-idx1-ubyte.gz' % self.file_pre)
        self.image_path = os.path.join(root, '%s-images-idx3-ubyte.gz' % self.file_pre)

        # Read file data and return pictures and labels.
        self.images, self.labels = self.__read_data__(self.image_path, self.label_path)

    def __read_data__(self, image_path, label_path):
        # Data set reading.
        with gzip.open(label_path, 'rb') as lbpath:
            labels = np.frombuffer(lbpath.read(), np.uint8, offset=8)
        with gzip.open(image_path, 'rb') as imgpath:
            images = np.frombuffer(imgpath.read(), np.uint8, offset=16).reshape(len(labels), 28, 28)
        return images, labels

    def __getitem__(self, index):
        image, label = self.images[index], int(self.labels[index])

        # If you need to convert to tensor, use tansform.
        if self.transform is not None:
            image = self.transform(np.array(image))  # Avoid bug: use np.array
        return image, label

    def __len__(self):
        return len(self.labels)
```

åŠ è½½æ–¹å¼ä¸ç¬¬ä¸€ç§æƒ…å†µç›¸åŒï¼Œä½†æ˜¯å°‘äº†downloadå‚æ•°ã€‚

#### [é€»è¾‘å›å½’æ¨¡å‹](./image_classification/logistic_regression.ipynb)

ç›®å‰å‡†ç¡®ç‡ï¼š92.17%

#### [å‰å‘ä¼ æ’­ç¥ç»ç½‘ç»œæ¨¡å‹](./image_classification/feedforward_neural_network.ipynb)

ç›®å‰å‡†ç¡®ç‡ï¼š97.16%

#### [ç®€å•å·ç§¯ç¥ç»ç½‘ç»œ](./image_classification/convolutional_neural_network.ipynb)

ç›®å‰å‡†ç¡®ç‡ï¼š98.8%

#### [LeNet-5](./image_classification/lenet-5.ipynb)

ç›®å‰å‡†ç¡®ç‡ï¼š99.04%

#### [RNN](./image_classification/recurrent_neural_network.ipynb)

ç›®å‰å‡†ç¡®ç‡ï¼š94.01%

### CIFAR10æ•°æ®é›†

[comparsion](./image_classification/comparison.ipynb)

```python
# Load downloaded dataset.(download=False)
train_dataset = torchvision.datasets.CIFAR10('data/CIFAR/', train=True, download=True, transform=transform_train)
test_dataset = torchvision.datasets.CIFAR10('data/CIFAR/', train=False, download=True, transform=transform_test)
```

#### [AlexNet](./image_classification/alexnet.ipynb)

ç›®å‰å‡†ç¡®ç‡ï¼š86.1%

#### [VGGNet](./image_classification/vggnet.ipynb)

VGGNetæ¨¡å‹æ€»çš„æ¥è¯´ï¼Œåˆ†ä¸ºVGG16å’ŒVGG19ä¸¤ç±»ï¼ŒåŒºåˆ«åœ¨äºæ¨¡å‹çš„å±‚æ•°ä¸åŒï¼Œä»¥ä¸‹'M'å‚æ•°ä»£è¡¨æ± åŒ–å±‚ï¼Œæ•°æ®ä»£è¡¨å„å±‚æ»¤æ³¢å™¨çš„æ•°é‡ã€‚

```python
# Define VGG-16 and VGG-19.
cfg = {
    'VGG-16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], 
    'VGG-19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512,'M']
}
```

ç›®å‰å‡†ç¡®ç‡ï¼š92.23%

`VGG-19`

ç›®å‰å‡†ç¡®ç‡ï¼š91.99%

#### [GoogLeNet](./image_classification/googlenet.ipynb)

```python
GoogLeNet(num_classes, aux_logits, init_weights)
```

å¦‚æœå¼€å¯è¾…åŠ©åˆ†ç±»å™¨ï¼Œé‚£ä¹ˆ`aux_logits=True`ï¼›å¦‚æœä¸å¼€å¯ï¼Œé‚£ä¹ˆ`aux_logits=False`ã€‚

`aux_logits=False`ï¼š

ç›®å‰å‡†ç¡®ç‡ï¼š85.88%

`aux_logits=True`ï¼š

ç›®å‰å‡†ç¡®ç‡ï¼š86.69%

#### [ResNet](./image_classification/resnet.ipynb)

ç›®å‰å‡†ç¡®ç‡ï¼š89.89%

## ç›®æ ‡æ£€æµ‹

> [å­¦ä¹ è·¯çº¿](https://blog.csdn.net/han_hhh/article/details/105906058)

### [YOLOv5sè§†é¢‘æ£€æµ‹](./object_detection/video_detection.ipynb)

## å‚è€ƒèµ„æ–™

* ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ï¼šé‚±é”¡é¹è‘—
* [æ·±åº¦å­¦ä¹ å®æˆ˜](https://github.com/Jack-Cherish/Deep-Learning)
* [PyTorchæ•™ç¨‹](https://www.w3cschool.cn/pytorch/)

## LICENSE
[MIT LICENSE](./LICENSE)